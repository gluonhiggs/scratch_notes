Docker is a way to package software so it can run on any hardware.

Nice sources to read about Docker:
https://towardsdatascience.com/learn-enough-docker-to-be-useful-b7ba70caeb4b
https://towardsdatascience.com/learn-enough-docker-to-be-useful-1c40ea269fa8

An article about hyperparameter search for ML or Spark configuration INSIGHT.
https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6
This is about how to enhance hyperparameter searching speed, not about improving flow performance.


YARN and Spark configuration for boosting performance preference
https://towardsdatascience.com/basics-of-apache-spark-configuration-settings-ca4faff40d45

Platform, Library, Framework, API, SDK, IDE distinction
https://shashvatshukla.medium.com/framework-vs-library-vs-platform-vs-api-vs-sdk-vs-toolkits-vs-ide-50a9473999db

Even though Data Engineer stack incesantly updates with new technologies, there are some philosophical cores of this stack:
https://towardsdatascience.com/the-new-data-engineering-stack-78939850bb30
https://towardsdatascience.com/why-you-should-consider-being-a-data-engineer-instead-of-a-data-scientist-2cf4e19dc019

Books:
https://towardsdatascience.com/5-books-for-data-engineers-f174bc1e7906

Data Pipeline:
https://towardsdatascience.com/lets-build-a-streaming-data-pipeline-e873d671fc57

ETLT Test:
https://blog.devgenius.io/how-to-integrate-data-quality-tests-in-the-python-etl-pipeline-359a535de564
