Docker is a way to package software so it can run on any hardware.

Nice sources to read about Docker:
https://towardsdatascience.com/learn-enough-docker-to-be-useful-b7ba70caeb4b
https://towardsdatascience.com/learn-enough-docker-to-be-useful-1c40ea269fa8

An article about hyperparameter search for ML or Spark configuration INSIGHT.
https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6
This is about how to enhance hyperparameter searching speed, not about improving flow performance.

Spark Performance
https://towardsdatascience.com/basics-of-apache-spark-configuration-settings-ca4faff40d45
https://medium.com/expedia-group-tech/part-3-efficient-executor-configuration-for-apache-spark-b4602929262

Handling large json files:
https://towardsdatascience.com/data-skew-in-pyspark-783d529a9dd7
https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8
https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def
https://towardsdatascience.com/interactively-analyse-100gb-of-json-data-with-spark-e018f9436e76

Platform, Library, Framework, API, SDK, IDE distinction
https://shashvatshukla.medium.com/framework-vs-library-vs-platform-vs-api-vs-sdk-vs-toolkits-vs-ide-50a9473999db

Even though Data Engineer stack incesantly updates with new technologies, there are some philosophical cores of this stack:
https://towardsdatascience.com/the-new-data-engineering-stack-78939850bb30
https://towardsdatascience.com/why-you-should-consider-being-a-data-engineer-instead-of-a-data-scientist-2cf4e19dc019

Books:
https://towardsdatascience.com/5-books-for-data-engineers-f174bc1e7906

Data Pipeline:
https://towardsdatascience.com/lets-build-a-streaming-data-pipeline-e873d671fc57

ETLT Test:
https://blog.devgenius.io/how-to-integrate-data-quality-tests-in-the-python-etl-pipeline-359a535de564
